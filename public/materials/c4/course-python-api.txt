×¡×§×¨×™×¤×˜ Python ×œ×—×™×‘×•×¨ ×œ-Gemini API
====================================

×§×•×“ Python ××œ× ×•××§×¦×•×¢×™
=======================

ğŸ ××‘×•×
=========

××“×¨×™×š ×–×” ××›×™×œ ×§×•×“ Python ××œ× ×•main.py ×œ×”×ª×—×œ×” ××”×™×¨×” ×¢× Gemini API.
×”×›×œ ××•×›×Ÿ ×œ×”×¨×¦×”!

××” ×›×œ×•×œ:
--------
âœ“ ×”×ª×§× ×” ×•××ª×—×•×œ
âœ“ ×¤×•× ×§×¦×™×•×ª ××•×›× ×•×ª
âœ“ Error handling
âœ“ Best practices
âœ“ ×“×•×’×××•×ª ×©×™××•×©
âœ“ Production ready

×“×¨×™×©×•×ª
========

Python Version:
---------------
â€¢ Python 3.8+
â€¢ pip (package manager)

×—×‘×™×œ×•×ª:
--------
```
google-generativeai
python-dotenv
pillow (×œ×ª××•× ×•×ª)
```

×”×ª×§× ×” ××”×™×¨×”:
-------------
```bash
pip install google-generativeai python-dotenv pillow
```

×”×’×“×¨×ª ×¡×‘×™×‘×”
============

×§×•×‘×¥ .env
---------

×¦×•×¨ ×§×•×‘×¥ `.env` ×‘×©×•×¨×© ×”×¤×¨×•×™×§×˜:

```
GEMINI_API_KEY=your_api_key_here
GEMINI_MODEL=gemini-1.5-flash
```

âš ï¸ ×”×•×¡×£ ×œ-.gitignore:
```
.env
__pycache__/
*.pyc
```

×§×•×‘×¥ requirements.txt
----------------------

```
google-generativeai>=0.3.0
python-dotenv>=1.0.0
Pillow>=10.0.0
```

×”×ª×§× ×”:
```bash
pip install -r requirements.txt
```

×”×§×•×“ ×”××œ×
==========

main.py - ×—×œ×§ 1: Imports ×•×”×’×“×¨×•×ª
----------------------------------

```python
"""
Gemini API Client
=================

×§×•×‘×¥ ×¨××©×™ ×œ×¢×‘×•×“×” ×¢× Gemini API.
×›×•×œ×œ ×¤×•× ×§×¦×™×•×ª ×‘×¡×™×¡×™×•×ª ×•××ª×§×“××•×ª.

Author: Google AI Academy
Version: 1.0.0
Date: 2026-01
"""

import os
import json
import logging
from typing import List, Dict, Optional, Union
from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai
from google.api_core import exceptions
from PIL import Image

# ×”×’×“×¨×ª ×œ×•×’×™×
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ×˜×¢×™× ×ª ××©×ª× ×™ ×¡×‘×™×‘×”
load_dotenv()

# ×”×’×“×¨×ª API Key
API_KEY = os.getenv('GEMINI_API_KEY')
if not API_KEY:
    raise ValueError("GEMINI_API_KEY not found in .env file")

# ×§×•× ×¤×™×’×•×¨×¦×™×”
genai.configure(api_key=API_KEY)

# ×§×‘×•×¢×™×
DEFAULT_MODEL = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
MAX_RETRIES = 3
TIMEOUT = 30
```

×—×œ×§ 2: ×”××—×œ×§×” ×”×¨××©×™×ª
----------------------

```python
class GeminiClient:
    """
    ××—×œ×§×” ×œ× ×™×”×•×œ ×—×™×‘×•×¨ ×œ-Gemini API.
    
    Attributes:
        model_name (str): ×©× ×”××•×“×œ
        generation_config (dict): ×”×’×“×¨×•×ª ×’'× ×¨×¦×™×”
        safety_settings (list): ×”×’×“×¨×•×ª ×‘×˜×™×—×•×ª
    """
    
    def __init__(
        self,
        model_name: str = DEFAULT_MODEL,
        temperature: float = 0.7,
        top_p: float = 0.95,
        top_k: int = 40,
        max_output_tokens: int = 2048
    ):
        """
        ××ª×—×•×œ ×”×œ×§×•×—.
        
        Args:
            model_name: ×©× ×”××•×“×œ
            temperature: ×¨××ª ×™×¦×™×¨×ª×™×•×ª (0-2)
            top_p: Top-p sampling
            top_k: Top-k sampling
            max_output_tokens: ××§×¡×™××•× ×˜×•×§× ×™× ×‘×ª×©×•×‘×”
        """
        self.model_name = model_name
        
        # ×”×’×“×¨×•×ª ×’'× ×¨×¦×™×”
        self.generation_config = {
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "max_output_tokens": max_output_tokens,
        }
        
        # ×”×’×“×¨×•×ª ×‘×˜×™×—×•×ª
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
        ]
        
        # ×™×¦×™×¨×ª ××•×“×œ
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=self.generation_config,
            safety_settings=self.safety_settings
        )
        
        logger.info(f"GeminiClient initialized with model: {self.model_name}")
    
    def generate(
        self,
        prompt: str,
        system_instruction: Optional[str] = None
    ) -> str:
        """
        ×™×¦×™×¨×ª ×ª×•×›×Ÿ ××˜×§×¡×˜.
        
        Args:
            prompt: ×”×¤×¨×•××¤×˜
            system_instruction: ×”×•×¨××•×ª ××¢×¨×›×ª (××•×¤×¦×™×•× ×œ×™)
            
        Returns:
            str: ×”×ª×©×•×‘×” ××”××•×“×œ
            
        Raises:
            exceptions.ResourceExhausted: ×× ×”×§×•×•××˜×” ×ª××”
            exceptions.InvalidArgument: ×× ×”×§×œ×˜ ×œ× ×ª×§×™×Ÿ
        """
        try:
            if system_instruction:
                model = genai.GenerativeModel(
                    model_name=self.model_name,
                    generation_config=self.generation_config,
                    safety_settings=self.safety_settings,
                    system_instruction=system_instruction
                )
                response = model.generate_content(prompt)
            else:
                response = self.model.generate_content(prompt)
            
            logger.info("Content generated successfully")
            return response.text
            
        except exceptions.ResourceExhausted:
            logger.error("API quota exceeded")
            raise
        except exceptions.InvalidArgument as e:
            logger.error(f"Invalid argument: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            raise
    
    def generate_with_image(
        self,
        prompt: str,
        image_path: Union[str, Path],
    ) -> str:
        """
        ×™×¦×™×¨×ª ×ª×•×›×Ÿ ××˜×§×¡×˜ ×•×ª××•× ×”.
        
        Args:
            prompt: ×”×¤×¨×•××¤×˜
            image_path: × ×ª×™×‘ ×œ×ª××•× ×”
            
        Returns:
            str: ×”×ª×©×•×‘×” ××”××•×“×œ
        """
        try:
            img = Image.open(image_path)
            response = self.model.generate_content([prompt, img])
            
            logger.info("Content with image generated successfully")
            return response.text
            
        except FileNotFoundError:
            logger.error(f"Image not found: {image_path}")
            raise
        except Exception as e:
            logger.error(f"Error generating with image: {e}")
            raise
    
    def generate_json(
        self,
        prompt: str,
        schema: Optional[Dict] = None
    ) -> Dict:
        """
        ×™×¦×™×¨×ª ×ª×•×›×Ÿ ×‘×¤×•×¨××˜ JSON.
        
        Args:
            prompt: ×”×¤×¨×•××¤×˜
            schema: ×¡×›××ª JSON (××•×¤×¦×™×•× ×œ×™)
            
        Returns:
            dict: ×”×ª×©×•×‘×” ×›-JSON
        """
        try:
            if schema:
                prompt = f"{prompt}\n\nReturn JSON following this schema:\n{json.dumps(schema, indent=2)}"
            
            # ×”×’×“×¨×ª ×ª×¦×•×¨×” ×œ-JSON
            config = self.generation_config.copy()
            config["response_mime_type"] = "application/json"
            
            model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config=config,
                safety_settings=self.safety_settings
            )
            
            response = model.generate_content(prompt)
            
            logger.info("JSON content generated successfully")
            return json.loads(response.text)
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            raise
        except Exception as e:
            logger.error(f"Error generating JSON: {e}")
            raise
    
    def stream_generate(self, prompt: str):
        """
        ×™×¦×™×¨×ª ×ª×•×›×Ÿ ×¢× streaming.
        
        Args:
            prompt: ×”×¤×¨×•××¤×˜
            
        Yields:
            str: ×—×œ×§×™ ×ª×©×•×‘×”
        """
        try:
            response = self.model.generate_content(
                prompt,
                stream=True
            )
            
            for chunk in response:
                if chunk.text:
                    yield chunk.text
            
            logger.info("Streaming completed successfully")
            
        except Exception as e:
            logger.error(f"Error during streaming: {e}")
            raise
```

×—×œ×§ 3: Chat Session
--------------------

```python
class ChatSession:
    """
    ××—×œ×§×” ×œ× ×™×”×•×œ ×©×™×—×” ×¢× ×–×™×›×¨×•×Ÿ.
    
    Attributes:
        client (GeminiClient): ×”×œ×§×•×—
        history (list): ×”×™×¡×˜×•×¨×™×™×ª ×”×©×™×—×”
    """
    
    def __init__(self, client: GeminiClient):
        """
        ××ª×—×•×œ ×¡×©×Ÿ ×¦'××˜.
        
        Args:
            client: GeminiClient ××•×¤×¢
        """
        self.client = client
        self.chat = client.model.start_chat(history=[])
        logger.info("Chat session started")
    
    def send_message(self, message: str) -> str:
        """
        ×©×œ×™×—×ª ×”×•×“×¢×” ×‘×¦'××˜.
        
        Args:
            message: ×”×”×•×“×¢×”
            
        Returns:
            str: ×”×ª×©×•×‘×”
        """
        try:
            response = self.chat.send_message(message)
            logger.info("Message sent successfully")
            return response.text
        except Exception as e:
            logger.error(f"Error sending message: {e}")
            raise
    
    def get_history(self) -> List[Dict]:
        """
        ×§×‘×œ×ª ×”×™×¡×˜×•×¨×™×™×ª ×”×©×™×—×”.
        
        Returns:
            list: ×”×™×¡×˜×•×¨×™×”
        """
        return [
            {
                "role": msg.role,
                "parts": [part.text for part in msg.parts]
            }
            for msg in self.chat.history
        ]
    
    def clear_history(self):
        """××—×™×§×ª ×”×™×¡×˜×•×¨×™×” ×•×”×ª×—×œ×” ××—×“×©."""
        self.chat = self.client.model.start_chat(history=[])
        logger.info("Chat history cleared")
```

×—×œ×§ 4: ×¤×•× ×§×¦×™×•×ª ×¢×–×¨
--------------------

```python
def list_models() -> List[str]:
    """
    ×§×‘×œ×ª ×¨×©×™××ª ××•×“×œ×™× ×–××™× ×™×.
    
    Returns:
        list: ×©××•×ª ××•×“×œ×™×
    """
    try:
        models = genai.list_models()
        model_names = [
            m.name for m in models
            if 'generateContent' in m.supported_generation_methods
        ]
        logger.info(f"Found {len(model_names)} models")
        return model_names
    except Exception as e:
        logger.error(f"Error listing models: {e}")
        return []

def count_tokens(text: str, model_name: str = DEFAULT_MODEL) -> int:
    """
    ×¡×¤×™×¨×ª ×˜×•×§× ×™× ×‘×˜×§×¡×˜.
    
    Args:
        text: ×”×˜×§×¡×˜
        model_name: ×©× ×”××•×“×œ
        
    Returns:
        int: ××¡×¤×¨ ×˜×•×§× ×™×
    """
    try:
        model = genai.GenerativeModel(model_name)
        result = model.count_tokens(text)
        return result.total_tokens
    except Exception as e:
        logger.error(f"Error counting tokens: {e}")
        return 0

def estimate_cost(
    input_tokens: int,
    output_tokens: int,
    model_name: str = DEFAULT_MODEL
) -> float:
    """
    ×”×¢×¨×›×ª ×¢×œ×•×ª ×œ×¤×™ ×˜×•×§× ×™×.
    
    Args:
        input_tokens: ×˜×•×§× ×™× ×§×œ×˜
        output_tokens: ×˜×•×§× ×™× ×¤×œ×˜
        model_name: ×©× ×”××•×“×œ
        
    Returns:
        float: ×¢×œ×•×ª ××©×•×¢×¨×ª ×‘-$
    """
    # ××—×™×¨×™× ×œ×“×•×’××” (2026)
    prices = {
        'gemini-1.5-flash': {
            'input': 0.00025 / 1000,
            'output': 0.0005 / 1000
        },
        'gemini-1.5-pro': {
            'input': 0.00125 / 1000,
            'output': 0.005 / 1000
        }
    }
    
    if model_name not in prices:
        model_name = 'gemini-1.5-flash'
    
    cost = (
        input_tokens * prices[model_name]['input'] +
        output_tokens * prices[model_name]['output']
    )
    
    return round(cost, 6)
```

×—×œ×§ 5: ×“×•×’×××•×ª ×©×™××•×©
----------------------

```python
def example_basic():
    """×“×•×’××” ×‘×¡×™×¡×™×ª."""
    print("\n=== Basic Example ===\n")
    
    client = GeminiClient()
    response = client.generate("Write a haiku about Python")
    print(response)

def example_with_system():
    """×“×•×’××” ×¢× System Instructions."""
    print("\n=== System Instructions Example ===\n")
    
    client = GeminiClient()
    response = client.generate(
        "Write a function to reverse a string",
        system_instruction="You are a Python expert. Always include docstrings."
    )
    print(response)

def example_json():
    """×“×•×’××” ×¢× JSON."""
    print("\n=== JSON Example ===\n")
    
    client = GeminiClient()
    schema = {
        "name": "string",
        "age": "number",
        "hobbies": "array of strings"
    }
    
    result = client.generate_json(
        "Create a profile for a fictional person",
        schema=schema
    )
    print(json.dumps(result, indent=2))

def example_chat():
    """×“×•×’××” ×¦'××˜."""
    print("\n=== Chat Example ===\n")
    
    client = GeminiClient()
    chat = ChatSession(client)
    
    print("User: Hi! My name is John")
    print(f"AI: {chat.send_message('Hi! My name is John')}")
    
    print("\nUser: What's my name?")
    print(f"AI: {chat.send_message('What is my name?')}")

def example_stream():
    """×“×•×’××” streaming."""
    print("\n=== Streaming Example ===\n")
    
    client = GeminiClient()
    print("AI: ", end='', flush=True)
    
    for chunk in client.stream_generate("Tell me a short story about AI"):
        print(chunk, end='', flush=True)
    
    print("\n")

def example_image():
    """×“×•×’××” ×¢× ×ª××•× ×”."""
    print("\n=== Image Example ===\n")
    
    # ×™×¦×™×¨×ª ×ª××•× ×” ×“××” (××• ×©×™××•×© ×‘×ª××•× ×” ×§×™×™××ª)
    img = Image.new('RGB', (100, 100), color='red')
    img.save('test.jpg')
    
    client = GeminiClient()
    response = client.generate_with_image(
        "What color is this image?",
        "test.jpg"
    )
    print(response)
    
    # ××—×™×§×ª ×”×§×•×‘×¥
    os.remove('test.jpg')
```

×—×œ×§ 6: Main
------------

```python
def main():
    """
    ×¤×•× ×§×¦×™×” ×¨××©×™×ª.
    """
    print("=" * 50)
    print("Gemini API Client - Examples")
    print("=" * 50)
    
    # ×”×¨×¦×ª ×›×œ ×”×“×•×’×××•×ª
    try:
        example_basic()
        example_with_system()
        example_json()
        example_chat()
        example_stream()
        # example_image()  # Uncomment to test
        
    except exceptions.ResourceExhausted:
        print("\nâš ï¸  API quota exceeded. Try again later.")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
    
    # ××™×“×¢ × ×•×¡×£
    print("\n" + "=" * 50)
    print("Available Models:")
    print("=" * 50)
    for model in list_models()[:5]:  # First 5
        print(f"  â€¢ {model}")
    
    print("\n" + "=" * 50)
    print("Done! âœ…")
    print("=" * 50)

if __name__ == "__main__":
    main()
```

×”×¨×¦×”
=====

×”×¨×¦×” ×‘×¡×™×¡×™×ª:
-------------
```bash
python main.py
```

Interactive Mode:
-----------------
```python
from main import GeminiClient, ChatSession

# ×™×¦×™×¨×ª ×œ×§×•×—
client = GeminiClient()

# ×©×™××•×© ×‘×¡×™×¡×™
response = client.generate("Hello!")
print(response)

# ×¦'××˜
chat = ChatSession(client)
print(chat.send_message("Hi!"))
```

Tips & Best Practices
======================

1. Error Handling
-----------------
âœ“ ×ª××™×“ ×¢×˜×•×£ ×‘-try-except
âœ“ Log ×©×’×™××•×ª
âœ“ Retry logic

2. Performance
--------------
âœ“ Cache ×ª×•×¦××•×ª
âœ“ Batch requests
âœ“ Flash ×œ×¤×©×•×˜

3. Security
-----------
âœ“ .env ×œ×¡×•×“×•×ª
âœ“ ×œ× ×œ×”×“×¤×™×¡ API Key
âœ“ Validate input

4. Monitoring
-------------
âœ“ Log ×©×™××•×©
âœ“ Track costs
âœ“ Monitor errors

5. Testing
----------
âœ“ Unit tests
âœ“ Integration tests
âœ“ Mock API calls

×¡×™×›×•×
=====

âœ“ ×§×•×“ ××œ× ×•×¢×•×‘×“
âœ“ Error handling ××§×™×£
âœ“ ×“×•×’×××•×ª ××¢×©×™×•×ª
âœ“ Best practices
âœ“ Production ready

×”×ª×—×œ ×œ×§×•×“! ğŸ

---
Â© 2026 Google AI Academy
Python API Client
×’×¨×¡×” 2.0
